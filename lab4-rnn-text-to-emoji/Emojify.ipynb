{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if matplotlib and emoji is not installed.\n",
    "# !pip install matplotlib\n",
    "# !pip install emoji\n",
    "\n",
    "# Uncomment the following lines if gluon-nlp, spacy is not installed.\n",
    "# !pip install gluon-nlp\n",
    "# !pip install spacy -U --quiet\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import csv\n",
    "import emoji\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MXNet Context. Use mx.cpu() for CPU. Use mx.gpu(0) for 1 GPU\n",
    "context = mx.cpu() #mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Pretrained wikitext-2 Language Model\n",
    "\n",
    "We use a pretrained model on wikitext-2 dataset [6]. Specifically, we use Vocabulary, Language Model i.e., Embeddings and Encodings (LSTM weights) based on wikitext-2 dataset.\n",
    "\n",
    "**Intuition:** Using pretrained language model weights is a common approach for semi-supervised learning in NLP. In order to do a good job with large language modeling on a large corpus of text, our model must learn representations that contain information about the structure of natural language. Intuitively, by starting with these good features, vs random features, we’re able to converge faster upon a good model for our downsteam task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_name = 'standard_lstm_lm_200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=True,\n",
    "                                      ctx=context,\n",
    "                                      dropout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data\n",
    "\n",
    "## 3.1 Read data\n",
    "\n",
    "* Load Emojify dataset\n",
    "* This is a tiny dataset (X, Y) where:\n",
    "       * X contains 127 sentences (strings) \n",
    "       * Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence\n",
    "\n",
    "    ![Emoji Dataset](./data_set.png)\n",
    "\n",
    "## 3.2 Process data and prepare dataset\n",
    "\n",
    "* Tokenize using spaCy- Extract words, punctuation marks from review text\n",
    "* Convert each token to an index in the vocabulary. Vocabulary is obtained from wikitext2\n",
    "* Prepare data iterators that can iterate on training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    \"\"\"Utility to read CSV data from given filename\n",
    "    \"\"\"\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji.append(row[1])\n",
    "    return phrase, emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('train_emoji.csv')\n",
    "X_test, Y_test = read_csv('test_emoji.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mx.gluon.data.dataset.ArrayDataset(X_train, Y_train)\n",
    "test_dataset = mx.gluon.data.dataset.ArrayDataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    \"\"\" Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    \"\"\"\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Train dataset...\n",
      "Preparing Test dataset...\n",
      "Data is ready!!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use spaCy English (en) tokenizer on input sentences to get tokens(words and punctuation marks)\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "\n",
    "# Clip sentences to be max 500 tokens\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def preprocess(x):\n",
    "    \"\"\"\n",
    "    1. Prepare labels. label = 1 (positive) if score > 5. label = 0 (negative) if score <= 5.\n",
    "    2. Tokenize - Extract words, punctuation marks from review text.\n",
    "    3. Convert each token to an index in the vocabulary.\n",
    "    \"\"\"\n",
    "    data, label = x\n",
    "\n",
    "    # Tokenize the data\n",
    "    tokenized_data = tokenizer(data)\n",
    "    # Clip the tokens\n",
    "    tokenized_clipped_data = length_clip(tokenized_data)\n",
    "    # Get vocabulary indexes for the tokens. Use pre-loaded 'vocab'.\n",
    "    data = vocab[tokenized_clipped_data]\n",
    "\n",
    "    return data, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    \n",
    "    return dataset, lengths\n",
    "\n",
    "# Preprocess the dataset\n",
    "print(\"Preparing Train dataset...\")\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "\n",
    "print(\"Preparing Test dataset...\")\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)\n",
    "\n",
    "print(\"Data is ready!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data -  they are so kind and friendly ❤️\n",
      "Processed Data -  ([56, 36, 143, 1735, 6, 4903], '0')\n"
     ]
    }
   ],
   "source": [
    "# Raw data - Text and label\n",
    "# Print a Test data\n",
    "index = 77\n",
    "print(\"Raw Data - \", X_train[index], label_to_emoji(Y_train[index]))\n",
    "\n",
    "# Processed data - Text is tokenized and converted to index of vocabulary \n",
    "print(\"Processed Data - \", train_dataset[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Prepare Dataloader\n",
    "\n",
    "* Input sentences can be of different lengths.\n",
    "* Use FixedBucketSampler, which assigns each data sample to a fixed bucket based on its length.\n",
    "* Batchify function (batchify) is applied on all the samples as the loaders read the batches.\n",
    "* We apply *Pad* for padding smaller length sequence to max length sequence in the bucket.\n",
    "* We apply *Stack* for stacking data, label, data_length i.e., [sentence, sentiment label, sentence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "bucket_num, bucket_ratio = 5, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=132, batch_num=10\n",
      "  key=[2, 4, 6, 8, 10]\n",
      "  cnt=[9, 61, 41, 15, 6]\n",
      "  batch_size=[16, 16, 16, 16, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "def get_dataloader():\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, ret_length=True),\n",
    "        nlp.data.batchify.Stack(dtype='float32'))\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn, num_workers=4)\n",
    "    test_dataloader = gluon.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn, num_workers=4)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Network\n",
    "\n",
    "* **Embedding, LSTM Layer:** To use pre-trained weights, we base our network on the Language Model Network (Embedding -> LSTM). \n",
    "* **Mean Pooling Layer:** We have multiple words input (reviews) and one output (sentiment). Hence, we average(mean) states across all time steps into one value.\n",
    "* **Dense Layer:** To generate the final output\n",
    "\n",
    "<img src=\"model.png\" width=\"400\" height=\"1100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition - Embedding\n",
    "\n",
    "<img src=\"embedding.png\" width=\"1000\" height=\"1100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolingLayer(gluon.HybridBlock):\n",
    "    \"\"\"A block for mean pooling of encoder features\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(MeanPoolingLayer, self).__init__(prefix=prefix, params=params)\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length):\n",
    "        masked_encoded = F.SequenceMask(data,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        agg_state = F.broadcast_div(F.sum(masked_encoded, axis=0),\n",
    "                                    F.expand_dims(valid_length, axis=1))\n",
    "        return agg_state\n",
    "\n",
    "\n",
    "class SentimentNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = None # will set with lm embedding later\n",
    "            self.encoder = None # will set with lm encoder later\n",
    "            self.agg_layer = MeanPoolingLayer()\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                # 5 possible emojis, hence, output 5 values from last layer.\n",
    "                self.output.add(gluon.nn.Dense(5, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): \n",
    "        embedded = self.embedding(data)\n",
    "        encoded = self.encoder(embedded)\n",
    "        agg_state = self.agg_layer(encoded, valid_length)\n",
    "        out = self.output(agg_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Initialize Network with Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2)\n",
      "  (agg_layer): MeanPoolingLayer(\n",
      "  \n",
      "  )\n",
      "  (output): HybridSequential(\n",
      "    (0): Dense(None -> 5, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SentimentNet()\n",
    "\n",
    "# Use Pretrained Embeddings from wikitext-2\n",
    "net.embedding = lm_model.embedding\n",
    "\n",
    "# Use Pretrained Encoder states (LSTM) from wikitext-2\n",
    "net.encoder = lm_model.encoder\n",
    "\n",
    "net.hybridize()\n",
    "\n",
    "# Random initialize the last Dense Laywer\n",
    "net.output.initialize(mx.init.Xavier(), ctx=context)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "epochs = 10\n",
    "grad_clip = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Evaluation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, context):\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    print('Begin Testing...')\n",
    "    for i, ((data, valid_length), label) in enumerate(dataloader):\n",
    "        # Step 1: Prepare data\n",
    "        data = mx.nd.transpose(data.as_in_context(context))\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        \n",
    "        # Step 2: Forward pass\n",
    "        output = net(data, valid_length)\n",
    "        \n",
    "        # Step 3: Calculate loss\n",
    "        L = loss(output, label)\n",
    "        \n",
    "        #print(\"output\")\n",
    "        #print(output)\n",
    "        # Step 4: Statistics - Keeping moving average loss and accuracy\n",
    "        pred =  nd.argmax(output, axis=1) #(output > 0.5).reshape(-1)\n",
    "        \n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    # Use Follow the Moving Leader Optimizer - [7]\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'ftml',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    parameters = net.collect_params().values()\n",
    "    print(\"Training the Emoji Prediction Model...\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        print(\"[Epoch - {}]\".format(epoch))\n",
    "        for i, ((data, length), label) in enumerate(train_dataloader):\n",
    "            L = 0\n",
    "            with autograd.record():\n",
    "                # Step 1: Forward pass\n",
    "                output = net(data.as_in_context(context).T,\n",
    "                             length.as_in_context(context)\n",
    "                                   .astype(np.float32))\n",
    "                # Step 2: Calculate Loss\n",
    "                L = L + loss(output, label.as_in_context(context)).mean()\n",
    "            \n",
    "            # Step 3: Backward pass\n",
    "            L.backward()\n",
    "            \n",
    "            # Step 3.1: Clip gradient - Avoid gradient explosion\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm(\n",
    "                    [p.grad(context) for p in parameters],\n",
    "                    grad_clip)\n",
    "            \n",
    "            # Step 4: Do parameter updates\n",
    "            trainer.step(1)\n",
    "            \n",
    "            # For epoch statistics - Loss and data sample count\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            epoch_L += L.asscalar()\n",
    "    \n",
    "        print('Train Avg Loss {:.6f}'.format(epoch_L / epoch_sent_num))\n",
    "        \n",
    "        # Step 5: Evaluation after each epoch\n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, context)\n",
    "        print('Test Acc {:.2f}, Test Avg Loss {:.6f}'.format(test_acc, test_avg_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Emoji Prediction Model...\n",
      "[Epoch - 0]\n",
      "Train Avg Loss 0.292042\n",
      "Begin Testing...\n",
      "Test Acc 0.41, Test Avg Loss 1.532183\n",
      "[Epoch - 1]\n",
      "Train Avg Loss 0.255741\n",
      "Begin Testing...\n",
      "Test Acc 0.41, Test Avg Loss 1.459041\n",
      "[Epoch - 2]\n",
      "Train Avg Loss 0.225316\n",
      "Begin Testing...\n",
      "Test Acc 0.43, Test Avg Loss 1.396440\n",
      "[Epoch - 3]\n",
      "Train Avg Loss 0.194800\n",
      "Begin Testing...\n",
      "Test Acc 0.48, Test Avg Loss 1.340150\n",
      "[Epoch - 4]\n",
      "Train Avg Loss 0.162051\n",
      "Begin Testing...\n",
      "Test Acc 0.48, Test Avg Loss 1.296086\n",
      "[Epoch - 5]\n",
      "Train Avg Loss 0.134145\n",
      "Begin Testing...\n",
      "Test Acc 0.45, Test Avg Loss 1.273903\n",
      "[Epoch - 6]\n",
      "Train Avg Loss 0.105193\n",
      "Begin Testing...\n",
      "Test Acc 0.48, Test Avg Loss 1.266124\n",
      "[Epoch - 7]\n",
      "Train Avg Loss 0.077812\n",
      "Begin Testing...\n",
      "Test Acc 0.55, Test Avg Loss 1.256138\n",
      "[Epoch - 8]\n",
      "Train Avg Loss 0.057923\n",
      "Begin Testing...\n",
      "Test Acc 0.55, Test Avg Loss 1.264227\n",
      "[Epoch - 9]\n",
      "Train Avg Loss 0.042038\n",
      "Begin Testing...\n",
      "Test Acc 0.57, Test Avg Loss 1.276389\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(net, context, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⚾'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['He', 'likes', 'playing', 'baseball']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "nd.argmax(prob1, axis=1).asscalar()\n",
    "label_to_emoji(int(nd.argmax(prob1, axis=1).asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'❤️'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['I', 'love', 'my', 'mother']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "nd.argmax(prob1, axis=1).asscalar()\n",
    "label_to_emoji(int(nd.argmax(prob1, axis=1).asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🍴'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['Lets', 'have', 'food', 'in', 'indian', 'restaurant']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "nd.argmax(prob1, axis=1).asscalar()\n",
    "label_to_emoji(int(nd.argmax(prob1, axis=1).asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😞'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['I', 'am', 'feeling', 'sick']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "nd.argmax(prob1, axis=1).asscalar()\n",
    "label_to_emoji(int(nd.argmax(prob1, axis=1).asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😄'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['He', 'is', 'funny']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "nd.argmax(prob1, axis=1).asscalar()\n",
    "label_to_emoji(int(nd.argmax(prob1, axis=1).asscalar()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "Problem statement and dataset is borrowed from [Coursera deeplearning.ai Sequence Models lecture](https://www.coursera.org/learn/nlp-sequence-models/home/welcome)\n",
    "\n",
    "# References\n",
    "\n",
    "1. http://mxnet.incubator.apache.org/\n",
    "2. https://gluon-nlp.mxnet.io\n",
    "3. https://spacy.io/usage/\n",
    "4. https://gluon-nlp.mxnet.io/examples/sentiment_analysis/sentiment_analysis.html\n",
    "5. http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "6. https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset\n",
    "7. https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.FTML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
